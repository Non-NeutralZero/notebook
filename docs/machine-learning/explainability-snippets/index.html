<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="version 2018

  Why Model interpretation?
  #

Understanding how a model makes decisions — model interpretation — has been on the front burner since the end of 2017. Decision support systems and models they are based on don’t explain which features influenced their decisions were known as black boxes.
Model interpretability is not only important for companies that need to fulfill legal obligations to customers. It serves a technical purpose as well. Every ML model considers input features (problem properties) to predict results (outputs). The more relevant features we create and use to train an ML model during feature engineering, the more accurate results we can get and the simpler our model is. That’s why the ability to understand how the model makes predictions is crucial for its debugging.
source"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://non-neutralzero.github.io/notebook/docs/machine-learning/explainability-snippets/"><meta property="og:site_name" content="Digital Notebook NonNeutralZero"><meta property="og:title" content="Digital Notebook NonNeutralZero"><meta property="og:description" content="version 2018
Why Model interpretation? # Understanding how a model makes decisions — model interpretation — has been on the front burner since the end of 2017. Decision support systems and models they are based on don’t explain which features influenced their decisions were known as black boxes. Model interpretability is not only important for companies that need to fulfill legal obligations to customers. It serves a technical purpose as well. Every ML model considers input features (problem properties) to predict results (outputs). The more relevant features we create and use to train an ML model during feature engineering, the more accurate results we can get and the simpler our model is. That’s why the ability to understand how the model makes predictions is crucial for its debugging. source"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Explainability Snippets | Digital Notebook NonNeutralZero</title>
<link rel=manifest href=/notebook/manifest.json><link rel=icon href=/notebook/favicon.png><link rel=stylesheet href=/notebook/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/notebook/flexsearch.min.js></script><script defer src=/notebook/en.search.min.fad218d11fc588acb32dd67e2dcc68c8a360d4cec34cb437a0f3ad1baf96db26.js integrity="sha256-+tIY0R/FiKyzLdZ+LcxoyKNg1M7DTLQ3oPOtG6+W2yY=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/notebook/><span>Digital Notebook NonNeutralZero</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-d70a1da63cb07790d45cd1070740b6fc class=toggle>
<label for=section-d70a1da63cb07790d45cd1070740b6fc class="flex justify-between"><a href=/notebook/docs/data-science/>Data Science</a></label><ul><li><a href=/notebook/docs/data-science/churn/churn-problem-framing/>Churn Problem Framing</a></li><li><a href=/notebook/docs/data-science/churn/churn/>Churn</a></li><li><a href=/notebook/docs/data-science/feature-engineering/feature-store/>Feature Store</a></li><li><a href=/notebook/docs/data-science/outlier-detection/>Outlier Detection</a></li><li><a href=/notebook/docs/data-science/sna/sna/>SNA</a></li></ul></li><li><input type=checkbox id=section-ea80d2588ab2432c68a3e3a3b032af95 class=toggle checked>
<label for=section-ea80d2588ab2432c68a3e3a3b032af95 class="flex justify-between"><a href=/notebook/docs/machine-learning/>Machine Learning</a></label><ul><li><a href=/notebook/docs/machine-learning/decision-trees/>Decision Trees</a></li><li><a href=/notebook/docs/machine-learning/explainability-snippets/ class=active>Explainability Snippets</a></li><li><a href=/notebook/docs/machine-learning/random-forests/>Random Forests</a></li><li><a href=/notebook/docs/machine-learning/svm/>Svm</a></li></ul></li><li><input type=checkbox id=section-fc1f94a4a6ef88f123f49cedfce9f479 class=toggle>
<label for=section-fc1f94a4a6ef88f123f49cedfce9f479 class="flex justify-between"><a href=/notebook/docs/deep-learning/>Deep Learning</a></label><ul><li><a href=/notebook/docs/deep-learning/logregnn/logregnn/>Log Reg Nn</a></li><li><a href=/notebook/docs/deep-learning/noprop/noprop/>No Prop</a></li></ul></li></ul><ul><li><a href=/notebook/posts/>Knowledge Base</a></li><li><a href=https://github.com/Non-NeutralZero target=_blank rel=noopener>Github</a></li><li><a href=https://nonneutralzero.com target=_blank rel=noopener>NonNeutralZero</a></li><li><a href=https://linktr.ee/nonneutralzero target=_blank rel=noopener>Linktree</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notebook/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Explainability Snippets</strong>
<label for=toc-control><img src=/notebook/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#why-model-interpretation>Why Model interpretation?</a></li><li><a href=#what-is-model-interpretability>What is model interpretability?</a></li><li><a href=#how-to-do-it>How to do it?</a></li><li><a href=#model-interpretation-guidelines>Model Interpretation Guidelines</a></li><li><a href=#resources>Resources</a></li></ul></li></ul></nav></aside></header><article class=markdown><p>version 2018</p><h3 id=why-model-interpretation>Why Model interpretation?
<a class=anchor href=#why-model-interpretation>#</a></h3><p>Understanding how a model makes decisions — model interpretation — has been on the front burner since the end of 2017. Decision support systems and models they are based on don’t explain which features influenced their decisions were known as black boxes.
Model interpretability is not only important for companies that need to fulfill legal obligations to customers. It serves a technical purpose as well. Every ML model considers input features (problem properties) to predict results (outputs). The more relevant features we create and use to train an ML model during feature engineering, the more accurate results we can get and the simpler our model is. That’s why the ability to understand how the model makes predictions is crucial for its debugging.
<a href=https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html>source</a></p><h3 id=what-is-model-interpretability>What is model interpretability?
<a class=anchor href=#what-is-model-interpretability>#</a></h3><p>Machine learning models are very complex tools, so-called black-box classifiers, that don’t offer straightforward and human-interpretable decision rules.</p><p>As data scientists, we should be able to provide an explanation to end users about how a model works. However, this not necessarily means understanding every piece of the model or generating a set of decision rules.
There could also be a case where this is not required:</p><ul><li>problem is well studied,</li><li>model results has no consequences,</li><li>understanding the model by the end-user could pose a risk of gaming the system.</li></ul><p><a href=https://www.kdnuggets.com/2019/05/interpretability-machine-learning-models.html>source</a></p><h3 id=how-to-do-it>How to do it?
<a class=anchor href=#how-to-do-it>#</a></h3><ul><li>Feature importance analysis</li><li>Feature correlations</li><li>Existing packages and tools (LIME, SHAP, Manifold&mldr;)</li></ul><h3 id=model-interpretation-guidelines>Model Interpretation Guidelines
<a class=anchor href=#model-interpretation-guidelines>#</a></h3><ul><li>Global Interpretability: How well can we understand the relationship between each feature and the predicted value at a global level — for our entire observation set. Can we understand both the magnitude and direction of the impact of each feature on the predicted value?</li><li>Local Interpretability: How well can we understand the relationship between each feature and the predicted value at a local level — for a specific observation.</li><li>Feature Selection: Does the model help us focus on only the features that matter? Can it zero out the features that are just “noise”?</li></ul><p>We can quickly identify that Feature X may be the most important, but does it make Outcome Y more or less likely? There may not be a yes-or-no answer.</p><h3 id=resources>Resources
<a class=anchor href=#resources>#</a></h3><h4 id=blogs-and-forums>Blogs and Forums
<a class=anchor href=#blogs-and-forums>#</a></h4><ul><li><a href=https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html>https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html</a></li><li><a href=https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f>https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f</a></li><li><a href=http://www.cse.chalmers.se/~richajo/dit866/lectures/l4/Feature%20ranking%20examples.html>http://www.cse.chalmers.se/~richajo/dit866/lectures/l4/Feature%20ranking%20examples.html</a></li><li><a href=https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/>https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/</a></li><li><a href=https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html>https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html</a></li><li><a href=https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html>https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html</a></li><li><a href=https://forums.fast.ai/t/feature-importance-of-random-forest-vs-xgboost/17561/2>https://forums.fast.ai/t/feature-importance-of-random-forest-vs-xgboost/17561/2</a></li><li><a href=https://github.com/parrt/random-forest-importances>https://github.com/parrt/random-forest-importances</a></li><li><a href=https://github.com/scikit-learn/scikit-learn/pull/13146>https://github.com/scikit-learn/scikit-learn/pull/13146</a></li><li><a href=https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7>https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7</a></li><li><a href=https://www.datascience.co.il/blog>https://www.datascience.co.il/blog</a></li><li><a href=https://christophm.github.io/interpretable-ml-book/>https://christophm.github.io/interpretable-ml-book/</a></li></ul><h4 id=papers>Papers
<a class=anchor href=#papers>#</a></h4><ul><li>“Why Should I Trust You?” Explaining the Predictions of Any Classifier <a href=https://arxiv.org/pdf/1602.04938.pdf>https://arxiv.org/pdf/1602.04938.pdf</a></li><li>LIME - an Introduction
<a href=https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime>https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime</a></li><li><a href=http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf>http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</a></li><li>Interpretable machine learning: definitions, methods, and applications
<a href=https://arxiv.org/pdf/1901.04592.pdf>https://arxiv.org/pdf/1901.04592.pdf</a></li><li><a href=https://web.stanford.edu/~hastie/Papers/ESLII.pdf>https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#why-model-interpretation>Why Model interpretation?</a></li><li><a href=#what-is-model-interpretability>What is model interpretability?</a></li><li><a href=#how-to-do-it>How to do it?</a></li><li><a href=#model-interpretation-guidelines>Model Interpretation Guidelines</a></li><li><a href=#resources>Resources</a></li></ul></li></ul></nav></div></aside></main></body></html>