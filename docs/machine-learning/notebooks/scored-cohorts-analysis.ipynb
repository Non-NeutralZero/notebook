{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scoredCohortsAnalysisSetup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zkFp5jdT2Y5",
        "colab_type": "text"
      },
      "source": [
        "I need a refactor!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE90M0HwPnow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly as py\n",
        "import time\n",
        "import json, urllib\n",
        "from sklearn import model_selection\n",
        "from time import mktime\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJRFzqT5QTI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_score_class(row):\n",
        "    \"\"\"\n",
        "    Creates a `scoring class/group` column, which is the one used in cohorts analysis, \n",
        "    based on the \"true score\".\n",
        "    Won't be easily generalized, depends on model and use case\n",
        "    \n",
        "    Example\n",
        "        df1[\"scoreClass\"] = df1.apply(add_score_class, axis = 1)\n",
        "    \"\"\"\n",
        "    if row['prediction2'] >= 0.95 :\n",
        "        val = \"High\"\n",
        "    elif row['prediction2'] >= 0.9 :\n",
        "        val = \"MHigh\"\n",
        "    elif row['prediction2'] >= 0.8 :\n",
        "        val = \"Medium\"\n",
        "    else:\n",
        "        val = \"Low\"\n",
        "    return val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXMPvtjnRTg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"\"\n",
        "project_folder = \"\"\n",
        "project_alias = \"\"\n",
        "my_scoring_dates_list =  [\"apr2019\", \"mar2019\", \"feb2019\"]\n",
        "\n",
        "def LoadCleanDataSet(scoring_dates_list) :\n",
        "    frames = []\n",
        "    for date in scoring_dates_list :\n",
        "        file = path + project_folder + project_alias +  date + \".csv\"\n",
        "        df = pd.read_csv(file,  sep=\"|\",  dtype={'0': np.str}, header = 0)\n",
        "        df[\"scoreDate\"] = my_scoring_dates_list[scoring_dates_list.index(date)]\n",
        "        df['scoreDate'] = df.scoreDate.apply(lambda x:datetime.fromtimestamp(mktime(time.strptime(x,'%b%Y'))))\n",
        "        df[\"scoreClass\"] = df.apply(add_score_class, axis = 1)\n",
        "        #df.columns = [str(col) + str(date) for col in df.columns]\n",
        "        ## Make sure df is observation distrinct\n",
        "        assert df['id' + str(date)].count() == df['id' + str(date)].drop_duplicates().count()\n",
        "        frames.append(df)\n",
        "    return frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2xQl05kR2il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from functools import partial, reduce\n",
        "# Process and structure into one dataframe\n",
        "## load\n",
        "loaded_dfs = LoadCleanDataSet(my_scoring_dates_list)\n",
        "\n",
        "## assemble\n",
        "concat_dfs = pd.concat(loaded_dfs , ignore_index=True)\n",
        "concat_dfs = dict(tuple(concat_dfs.groupby([concat_dfs['scoreDate'].dt.year,\n",
        "                                            concat_dfs['scoreDate'].dt.month])))\n",
        "concat_dfs_reduce = partial(pd.merge, on = \"id\", how='outer')\n",
        "ids_df  = reduce(concat_dfs_reduce, concat_dfs.values())\n",
        "## clean\n",
        "date_cols = [datecol for datecol in list(ids_df.columns) if ('Date') in datecol]\n",
        "scoreClass_cols = [classcol for classcol in list(ids_df.columns) if ('Class') in classcol]\n",
        "\n",
        "for i in range(len(date_cols)):\n",
        "    ids_df.loc[ids_df[date_cols[i]].isnull(),date_cols[i]] = ids_df[date_cols[i]].dropna().unique()[0]\n",
        "\n",
        "for i in range(len(scoreClass_cols)):\n",
        "    ids_df.loc[ids_df[scoreClass_cols[i]].isnull(),scoreClass_cols[i]] = 'Unscored'\n",
        "\n",
        "## done\n",
        "print(\"Completed: ids_df {0} ready!\".format(ids_df.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}