<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Random Forest
  #

Random forests are built from decision trees

Initially, the original data is bootstrapped by randomly sampling the data and creating a new dataset with the same size as the original one (to be able to do that, duplicated obs are allowed - aka random sampling with replacement)
Build a decision tree based on the bootsrapped data
Randomly select features (typically sqrt(n_features)) from the bootsrapped data when splitting nodes (this is called random subspace method)
Go back to step 1 and repeat



does all the original data end up in the sampled subsets? For each created Decision Tree, the non-bootsrapped data is called Out-of-Bag data.
once we get the forest, how do we use it? if we want to get a prediction, we run an obs through all the trees of the forest and pick the prediction with the most votes. (this process is called Bagging, i.e. Bootsrapping + aggregating single predictions)
how do we evaluate the random forest? we can evaluate it using the out-of-bag error, i.e. measure how accurate the forest predicts out-of-bag data.
is there an optimal number of features for each bootsrapped sample? Yes. Given that we can measure the out-of-bag error, we can use it to compare forests built on different samples of features and select the one with the smallest error.
how many times should we repeat this processes? plot OOB error rate vs. number of trees
Why are they called random forests? Because of the random sampling concept at step 1 and at step 3
how is a forest better than one decision tree? By getting a large number of different (high variance ) trees

For more, see Chapter 15 in https://web.stanford.edu/~hastie/Papers/ESLII.pdf"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://non-neutralzero.github.io/notebook/docs/machine-learning/random-forests/"><meta property="og:site_name" content="Digital Notebook NonNeutralZero"><meta property="og:title" content="Digital Notebook NonNeutralZero"><meta property="og:description" content=" Random Forest # Random forests are built from decision trees
Initially, the original data is bootstrapped by randomly sampling the data and creating a new dataset with the same size as the original one (to be able to do that, duplicated obs are allowed - aka random sampling with replacement) Build a decision tree based on the bootsrapped data Randomly select features (typically sqrt(n_features)) from the bootsrapped data when splitting nodes (this is called random subspace method) Go back to step 1 and repeat does all the original data end up in the sampled subsets? For each created Decision Tree, the non-bootsrapped data is called Out-of-Bag data. once we get the forest, how do we use it? if we want to get a prediction, we run an obs through all the trees of the forest and pick the prediction with the most votes. (this process is called Bagging, i.e. Bootsrapping + aggregating single predictions) how do we evaluate the random forest? we can evaluate it using the out-of-bag error, i.e. measure how accurate the forest predicts out-of-bag data. is there an optimal number of features for each bootsrapped sample? Yes. Given that we can measure the out-of-bag error, we can use it to compare forests built on different samples of features and select the one with the smallest error. how many times should we repeat this processes? plot OOB error rate vs. number of trees Why are they called random forests? Because of the random sampling concept at step 1 and at step 3 how is a forest better than one decision tree? By getting a large number of different (high variance ) trees For more, see Chapter 15 in https://web.stanford.edu/~hastie/Papers/ESLII.pdf"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Random Forests | Digital Notebook NonNeutralZero</title>
<link rel=manifest href=/notebook/manifest.json><link rel=icon href=/notebook/favicon.png><link rel=stylesheet href=/notebook/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/notebook/flexsearch.min.js></script><script defer src=/notebook/en.search.min.9fa39ba776820438cb27460505c4e0d8c380adacef78d782a10139f8f171ba6e.js integrity="sha256-n6Obp3aCBDjLJ0YFBcTg2MOArazveNeCoQE5+PFxum4=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/notebook/><span>Digital Notebook NonNeutralZero</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-d70a1da63cb07790d45cd1070740b6fc class=toggle>
<label for=section-d70a1da63cb07790d45cd1070740b6fc class="flex justify-between"><a href=/notebook/docs/data-science/>Data Science</a></label><ul><li><a href=/notebook/docs/data-science/churn/churn-management/>Churn Management</a></li><li><a href=/notebook/docs/data-science/churn/churn-problem-framing/>Churn Problem Framing</a></li><li><a href=/notebook/docs/data-science/feature-engineering/feature-store/>Feature Store</a></li><li><a href=/notebook/docs/data-science/outlier-detection/>Outlier Detection</a></li><li><a href=/notebook/docs/data-science/sna/sna/>SNA</a></li></ul></li><li><input type=checkbox id=section-ea80d2588ab2432c68a3e3a3b032af95 class=toggle checked>
<label for=section-ea80d2588ab2432c68a3e3a3b032af95 class="flex justify-between"><a role=button>Machine Learning</a></label><ul><li><a href=/notebook/docs/machine-learning/decision-trees/>Decision Trees</a></li><li><a href=/notebook/docs/machine-learning/explainability-snippets/>Explainability Snippets</a></li><li><a href=/notebook/docs/machine-learning/random-forests/ class=active>Random Forests</a></li><li><a href=/notebook/docs/machine-learning/svm/>Svm</a></li></ul></li><li><input type=checkbox id=section-fc1f94a4a6ef88f123f49cedfce9f479 class=toggle>
<label for=section-fc1f94a4a6ef88f123f49cedfce9f479 class="flex justify-between"><a href=/notebook/docs/deep-learning/>Deep Learning</a></label><ul><li><a href=/notebook/docs/deep-learning/logregnn/logregnn/>Log Reg Nn</a></li><li><a href=/notebook/docs/deep-learning/noprop/noprop/>No Prop</a></li></ul></li></ul><ul><li><a href=/notebook/posts/>Knowledge Base</a></li><li><a href=https://github.com/Non-NeutralZero target=_blank rel=noopener>Github</a></li><li><a href=https://nonneutralzero.com target=_blank rel=noopener>NonNeutralZero</a></li><li><a href=https://linktr.ee/nonneutralzero target=_blank rel=noopener>Linktree</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notebook/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Random Forests</strong>
<label for=toc-control><img src=/notebook/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class=markdown><h1 id=random-forest>Random Forest
<a class=anchor href=#random-forest>#</a></h1><p>Random forests are built from decision trees</p><ol><li>Initially, the original data is bootstrapped by randomly sampling the data and creating a new dataset with the same size as the original one (to be able to do that, duplicated obs are allowed - aka random sampling with replacement)</li><li>Build a decision tree based on the bootsrapped data</li><li>Randomly select features (typically sqrt(n_features)) from the bootsrapped data when splitting nodes (this is called random subspace method)</li><li>Go back to step 1 and repeat</li></ol><ul><li><em><strong>does all the original data end up in the sampled subsets?</strong></em> For each created Decision Tree, the non-bootsrapped data is called <strong>Out-of-Bag</strong> data.</li><li><em><strong>once we get the forest, how do we use it?</strong></em> if we want to get a prediction, we run an obs through all the trees of the forest and pick the prediction with the most votes. (this process is called <strong>Bagging</strong>, i.e. <strong>B</strong>ootsrapping + <strong>agg</strong>regating single predictions)</li><li><em><strong>how do we evaluate the random forest?</strong></em> we can evaluate it using the out-of-bag error, i.e. measure how accurate the forest predicts out-of-bag data.</li><li><em><strong>is there an optimal number of features for each bootsrapped sample?</strong></em> Yes. Given that we can measure the out-of-bag error, we can use it to compare forests built on different samples of features and select the one with the smallest error.</li><li><em><strong>how many times should we repeat this processes?</strong></em> plot OOB error rate vs. number of trees</li><li><em><strong>Why are they called random forests?</strong></em> Because of the random sampling concept at step 1 and at step 3</li><li><em><strong>how is a forest better than one decision tree?</strong></em> By getting a large number of different (high variance ) trees</li></ul><p>For more, see Chapter 15 in <a href=https://web.stanford.edu/~hastie/Papers/ESLII.pdf>https://web.stanford.edu/~hastie/Papers/ESLII.pdf</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>