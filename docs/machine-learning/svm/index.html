<!doctype html><html lang=en-us dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  SVM
  #


P: We want to figure out a way to separate data into classes
S: A linear classifier can help. Its objective would be to divide data using a hyperplane, and since the data points, from each class, that are closer to the classifier will be helping us decide on the orientation and position of the classifier, we can give them a fancy name (Support vectors) and call our linear classifier, the Support Vector Classifier.
P: But, If the data is &ldquo;clearly&rdquo; separable, then we won&rsquo;t have one classifier - we would actually end up with so many possibilities to choosing one hyperplane AND we have to make sure future predictions are more likely to be correctly classified
S: We could choose the one that insures the maximum distance between the Support vectors of the two classes (we&rsquo;ll call it a maximum margin classifier.) That way, we&rsquo;ll seperate the existing data, and have more confidence in classifying future data points.
P: We&rsquo;ll be looking to maximize the margin between the data points and the hyperplane, how do we do that?
S: Hinge Loss!
P: Overfitting, this type of classifier would be very sensitive to outliers for example
S: Explorative data analysis, outliers analysis or allow misclassification (soft margin). o
"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="https://non-neutralzero.github.io/notebook/docs/machine-learning/svm/"><meta property="og:site_name" content="Digital Notebook NonNeutralZero"><meta property="og:title" content="Digital Notebook NonNeutralZero"><meta property="og:description" content=" SVM # P: We want to figure out a way to separate data into classes S: A linear classifier can help. Its objective would be to divide data using a hyperplane, and since the data points, from each class, that are closer to the classifier will be helping us decide on the orientation and position of the classifier, we can give them a fancy name (Support vectors) and call our linear classifier, the Support Vector Classifier. P: But, If the data is “clearly” separable, then we won’t have one classifier - we would actually end up with so many possibilities to choosing one hyperplane AND we have to make sure future predictions are more likely to be correctly classified S: We could choose the one that insures the maximum distance between the Support vectors of the two classes (we’ll call it a maximum margin classifier.) That way, we’ll seperate the existing data, and have more confidence in classifying future data points. P: We’ll be looking to maximize the margin between the data points and the hyperplane, how do we do that? S: Hinge Loss! P: Overfitting, this type of classifier would be very sensitive to outliers for example S: Explorative data analysis, outliers analysis or allow misclassification (soft margin). o "><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Svm | Digital Notebook NonNeutralZero</title>
<link rel=manifest href=/notebook/manifest.json><link rel=icon href=/notebook/favicon.png><link rel=stylesheet href=/notebook/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz+OQteg=" crossorigin=anonymous><script defer src=/notebook/flexsearch.min.js></script><script defer src=/notebook/en.search.min.15683d1002750f1fcbabc7093bc6e20e485ac642156d55e391192c3a4abae0b6.js integrity="sha256-FWg9EAJ1Dx/Lq8cJO8biDkhaxkIVbVXjkRksOkq64LY=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/notebook/><span>Digital Notebook NonNeutralZero</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-d70a1da63cb07790d45cd1070740b6fc class=toggle>
<label for=section-d70a1da63cb07790d45cd1070740b6fc class="flex justify-between"><a href=/notebook/docs/data-science/>Data Science</a></label><ul><li><a href=/notebook/docs/data-science/churn/churn-management/>Churn Management</a></li><li><a href=/notebook/docs/data-science/churn/churn-problem-framing/>Churn Problem Framing</a></li><li><a href=/notebook/docs/data-science/feature-engineering/feature-store/>Feature Store</a></li><li><a href=/notebook/docs/data-science/outlier-detection/>Outlier Detection</a></li><li><a href=/notebook/docs/data-science/sna/sna/>SNA</a></li></ul></li><li><input type=checkbox id=section-ea80d2588ab2432c68a3e3a3b032af95 class=toggle checked>
<label for=section-ea80d2588ab2432c68a3e3a3b032af95 class="flex justify-between"><a role=button>Machine Learning</a></label><ul><li><a href=/notebook/docs/machine-learning/decision-trees/>Decision Trees</a></li><li><a href=/notebook/docs/machine-learning/explainability-snippets/>Explainability Snippets</a></li><li><a href=/notebook/docs/machine-learning/random-forests/>Random Forests</a></li><li><a href=/notebook/docs/machine-learning/svm/ class=active>Svm</a></li></ul></li><li><input type=checkbox id=section-fc1f94a4a6ef88f123f49cedfce9f479 class=toggle>
<label for=section-fc1f94a4a6ef88f123f49cedfce9f479 class="flex justify-between"><a href=/notebook/docs/deep-learning/>Deep Learning</a></label><ul><li><a href=/notebook/docs/deep-learning/logregnn/logregnn/>Log Reg Nn</a></li><li><a href=/notebook/docs/deep-learning/noprop/noprop/>No Prop</a></li></ul></li></ul><ul><li><a href=/notebook/posts/>Knowledge Base</a></li><li><a href=https://github.com/Non-NeutralZero target=_blank rel=noopener>Github</a></li><li><a href=https://nonneutralzero.com target=_blank rel=noopener>NonNeutralZero</a></li><li><a href=https://linktr.ee/nonneutralzero target=_blank rel=noopener>Linktree</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/notebook/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Svm</strong>
<label for=toc-control><img src=/notebook/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents></nav></aside></header><article class=markdown><h1 id=svm>SVM
<a class=anchor href=#svm>#</a></h1><ul><li>P: <em><strong>We want to figure out a way to separate data into classes</strong></em></li><li>S: A linear classifier can help. Its objective would be to divide data using a hyperplane, and since the data points, from each class, that are closer to the classifier will be helping us decide on the orientation and position of the classifier, we can give them a fancy name (Support vectors) and call our linear classifier, the Support Vector Classifier.</li><li>P: <em><strong>But, If the data is &ldquo;clearly&rdquo; separable, then we won&rsquo;t have one classifier - we would actually end up with so many possibilities to choosing one hyperplane AND we have to make sure future predictions are more likely to be correctly classified</strong></em></li><li>S: We could choose the one that insures the maximum distance between the Support vectors of the two classes (we&rsquo;ll call it a <strong>maximum margin classifier</strong>.) That way, we&rsquo;ll seperate the existing data, and have more confidence in classifying future data points.</li><li>P: <em><strong>We&rsquo;ll be looking to maximize the margin between the data points and the hyperplane, how do we do that?</strong></em></li><li>S: Hinge Loss!</li><li>P: <em><strong>Overfitting, this type of classifier would be very sensitive to outliers for example</strong></em></li><li>S: Explorative data analysis, outliers analysis or allow misclassification (soft margin). o</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents></nav></div></aside></main></body></html>