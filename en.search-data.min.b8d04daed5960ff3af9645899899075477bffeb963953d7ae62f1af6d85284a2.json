[{"id":0,"href":"/notebook/docs/data-science/","title":"Data Science","section":"Docs","content":" Data Science # Churn Management version du 25-12-2018\nGestion de l\u0026rsquo;attrition ou Churn management # Le churn est un anglicisme qui désigne la perte des clients. La gestion du churn figure parmi les piliers les plus importants à tenir en compte lors de la gestion de fidélisation, car l’acquisition à elle seule ne peut pallier aux pertes de la clientèle. (voir illustration)\nÉtudier le churn pousse à réfléchir parallèlement à plusieurs actions pouvant contribuer à sa réduction et à la maximisation de la valeur de la relation client:\nChurn Problem Framing Churn - Problem Framing # Churn reporting # What happened? (Inactive point of view)\nIn general cohorts work good here: start with a small population of churners and follow them over time Clustering churners based on project and modeling bottom line Example of small population of churners =\u0026gt; “Churners who generate more profits, churns who explicitly expressed their insatisfaction”: or choose another prioritization criterion Monetary retention rates (monetary churn) Reversed cohorts: start with the churn date (Month / Year) and follow the churners retrospectively Product / offers / segments cohorts Maybe change of nomenclature/perspective to make problem framing more accessible to other shareholders (Product Churn, Account Churn, Usage Churn\u0026hellip;etc) Why did it happen?\nFeature Store version 19-08-2024\nFeature Store as a System # follows a logical flow seperates concerns functionally builds modular-ily Pipeline and code break-down # Data Selection and UC-specific processing # We start by filtering a reference table based on specific conditions aligned with the requirements of the use case. This creates a use case table on which we select and process pivot columns.\nNote: This should happen in the processing and modeling modules. Pivot transformations and Data federation # We select specific columns in order to create transitional tables with the desired pivot columns.\nOutlier Detection Types of outliers # With regards to the distribution # Univariate: can be found when looking at a distribution of values in a single feature space. Multivariate: can be found in a n-dimensional space (of n-features). With regards to the environment # Point outliers: single data points that lay far from the rest of the distribution. Contextual outliers: can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition. Collective outliers: Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena. Most common causes of outliers on a data set # Data entry errors (human errors) Measurement errors (instrument errors) Experimental errors (data extraction or experiment planning/executing errors) Intentional (dummy outliers made to test detection methods) Data processing errors (data manipulation or data set unintended mutations) Sampling errors (extracting or mixing data from wrong or various sources) Natural (not an error, novelties in data) Hint\nSNA "},{"id":1,"href":"/notebook/docs/deep-learning/","title":"Deep Learning","section":"Docs","content":" Deep Learning # "},{"id":2,"href":"/notebook/posts/databricks-naming-convention/","title":"Databricks Naming Conventions","section":"Knowledge Base","content":" Introduction # Consistent naming across env (dev, test, prod), layers (bronze/silver/gold), and domains is critical in Databricks. It prevents confusion, enforces governance, and supports automation with Unity Catalog and Delta Lake.\nGeneral Best Practices # Separate dev / test / prod workspaces. Apply RBAC + Unity Catalog. Use modular notebooks; reuse with %run. Version control all code. Prefer job clusters; auto-terminate. Vacuum Delta tables; use optimize + z-order. Allow schema evolution only when intentional. Environment‑Aware Medallion Naming # Unity Catalog is the governance backbone. Inconsistent names break access policies and automation. Use env prefixes, clear domains, and snake_case (Unity Catalog docs .\nPattern:\n\u0026lt;env\u0026gt;_\u0026lt;domain\u0026gt; Examples: prod_sales, dev_marketing, test_finance\nLayer‑Specific Schemas # Pattern:\n\u0026lt;env\u0026gt;_\u0026lt;domain\u0026gt;.\u0026lt;layer\u0026gt; Examples: prod_sales.bronze, prod_sales.silver, prod_sales.gold\nTable Naming Within Layers # Use snake_case, descriptive names.\nbronze.transactions_raw silver.customer_validated gold.sales_monthly Full name example: prod_sales.bronze.transactions_raw\nFile Storage Structure # Mirror env/layer/domain/table in paths.\n/mnt/data/\u0026lt;env\u0026gt;/\u0026lt;layer\u0026gt;/\u0026lt;domain\u0026gt;/\u0026lt;table\u0026gt;/ Example: /mnt/data/prod/bronze/sales/transactions/\nSummary Table # Level Pattern Example Catalog \u0026lt;env\u0026gt;_\u0026lt;domain\u0026gt; dev_hr, prod_sales Schema / Layer \u0026lt;catalog\u0026gt;.\u0026lt;layer\u0026gt; test_finance.bronze Table Name snake_case silver.employee_cleaned Full Name \u0026lt;catalog\u0026gt;.\u0026lt;layer\u0026gt;.\u0026lt;table\u0026gt; prod_sales.gold.monthly_rev Storage Path /mnt/data/\u0026lt;env\u0026gt;/\u0026lt;layer\u0026gt;/\u0026lt;domain\u0026gt;/\u0026lt;table\u0026gt;/ /mnt/data/dev/bronze/marketing/ads/ Good vs Bad Examples # Workspaces # Good: dev, test, prod separated (pre-prod when maturity allows). Bad: mixed single workspace. Clusters # Good: job clusters with auto-termination. Bad: idle interactive cluster. Schemas / Layers # Good: prod_sales.bronze. Bad: bronze1, myschema. Tables # Good: silver.customer_validated. Bad: CustomerData, table1. Storage Paths # Good: /mnt/data/prod/gold/finance/revenue_summary/. Bad: /mnt/data/finaltables/finance2024/. Community Discussions # Link Post Date Latest Reply Reddit: r/databricks – Naming standards Confusion scaling naming Dec 2023 Jan 2024 Reddit: r/dataengineering – Unity Catalog naming Environment \u0026ldquo;grown wild\u0026rdquo; Nov 2023 Dec 2023 Reddit: r/dataengineering – Organizing Unity Catalog Catalog-first vs domain-first Aug 2024 Aug 2024 Stack Overflow – Catalog vs Database Terminology confusion Feb 2024 Feb 2024 Medium – Unity Catalog Principles Governance risks from inconsistency Sep 2023 Sep 2023 Medium – Best Practices Domain separation pitfalls Oct 2023 Oct 2023 Medium – Ultimate Guide Confusion when diverging from snake_case Jul 2024 Jul 2024 References # Official Docs\nDatabricks Unity Catalog Azure Databricks naming best practices Databricks Medallion Architecture Azure Q\u0026amp;A – Organizing multiple source systems Community \u0026amp; Practitioner Insights\ncarlosacchi.cloud – Ultimate Guide medium.com – Valentin Loghin reddit.com – Naming standards discussion "},{"id":3,"href":"/notebook/posts/multimodulewithsbt/","title":"Multi-module build based on sbt","section":"Knowledge Base","content":"import sbt.{Compile, Test, *} import Keys.{baseDirectory, libraryDependencies, *} // sbt.version = 1.6.2 ThisBuild / trackInternalDependencies := TrackLevel.TrackIfMissing lazy val welcome = taskKey[Unit](\u0026#34;welcome\u0026#34;) val sparkVersion = \u0026#34;2.4.0-cdh6.2.1\u0026#34; val hiveVersion = \u0026#34;2.1.1-cdh6.2.1\u0026#34; lazy val commonSettings = Seq( //organization := \u0026#34;com.nnz\u0026#34;, version := \u0026#34;0.1.0-SNAPSHOT\u0026#34;, welcome := { println(\u0026#34;Welcome !\u0026#34;)}, scalaVersion := \u0026#34;2.11.12\u0026#34;, javacOptions ++= Seq(\u0026#34;-source\u0026#34;, \u0026#34;15.0.10\u0026#34;, \u0026#34;-target\u0026#34;, \u0026#34;15.0.10\u0026#34;), libraryDependencies ++= sparkDependencies, resolvers ++= Seq(\u0026#34;Cloudera Versions\u0026#34; at \u0026#34;https://repository.cloudera.com/artifactory/cloudera-repos/\u0026#34;, ) ) lazy val root = (project in file(\u0026#34;.\u0026#34;)) .settings( name := \u0026#34;multimodule-project\u0026#34;, commonSettings, update / aggregate := true, ) .aggregate(warehouse, ingestion, processing) lazy val warehouse = (project in file(\u0026#34;warehouse\u0026#34;)) .settings( name := \u0026#34;warehouse\u0026#34;, commonSettings, Compile / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;main\u0026#34; / \u0026#34;scala\u0026#34;, Test / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;test\u0026#34; / \u0026#34;scala\u0026#34;, ) lazy val ingestion = (project in file(\u0026#34;ingestion\u0026#34;)) .dependsOn(warehouse) .settings( name := \u0026#34;ingestion\u0026#34;, commonSettings, Compile / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;main\u0026#34; / \u0026#34;scala\u0026#34;, Test / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;test\u0026#34; / \u0026#34;scala\u0026#34;, ) lazy val processing = (project in file(\u0026#34;processing\u0026#34;)) .dependsOn(warehouse, ingestion) .settings( name := \u0026#34;processing\u0026#34;, commonSettings, Compile / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;main\u0026#34; / \u0026#34;scala\u0026#34;, Test / scalaSource := baseDirectory.value /\u0026#34;.\u0026#34; / \u0026#34;src\u0026#34; / \u0026#34;test\u0026#34; / \u0026#34;scala\u0026#34;, ) /** * Spark Dependencies */ val sparkCore = \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-core\u0026#34; % sparkVersion val sparkSQL = \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-sql\u0026#34; % sparkVersion val sparkHive = \u0026#34;org.apache.spark\u0026#34; %% \u0026#34;spark-hive\u0026#34; % sparkVersion lazy val sparkDependencies = Seq(sparkCore, sparkSQL, sparkHive) https://gist.github.com/Non-NeutralZero/d5be154ee38962176bcc0bf49182c691\n"},{"id":4,"href":"/notebook/posts/latex-misc/","title":"LaTex","section":"Knowledge Base","content":" BibTex # Limit number of authors in IEEEtran # In the .bib file configure your IEEEtran as follows:\n@IEEEtranBSTCTL{IEEEexample:BSTcontrol, CTLuse_forced_etal = \u0026#34;yes\u0026#34;, CTLmax_names_forced_etal = \u0026#34;3\u0026#34;, CTLnames_show_etal = \u0026#34;2\u0026#34; } Cheat-sheets # Overleaf, Bibliography management with bibtex Sébastien Merkel, Reference sheet for natbib usage LaTeX/Bibliography Management. (2023, June 5). Wikibooks. Discipline Specific Listings of BibTeX Journal Styles "},{"id":5,"href":"/notebook/posts/jupyter-misc/","title":"Jupyter","section":"Knowledge Base","content":" Memory Usage # def memory(): with open(\u0026#39;/proc/meminfo\u0026#39;, \u0026#39;r\u0026#39;) as mem: ret = {} tmp = 0 for i in mem: sline = i.split() if str(sline[0]) == \u0026#39;MemTotal:\u0026#39;: ret[\u0026#39;total\u0026#39;] = int(sline[1]) elif str(sline[0]) in (\u0026#39;MemFree:\u0026#39;, \u0026#39;Buffers:\u0026#39;, \u0026#39;Cached:\u0026#39;): tmp += int(sline[1]) ret[\u0026#39;free\u0026#39;] = tmp ret[\u0026#39;used\u0026#39;] = int(ret[\u0026#39;total\u0026#39;]) - int(ret[\u0026#39;free\u0026#39;]) return ret No Hang Up # nohup jupyter notebook --no-browser \u0026gt; notebook.log 2\u0026gt;\u0026amp;1 \u0026amp; Workaround: no cells output # se = time.time() print(train.rdd.getNumPartitions()) print(test.rdd.getNumPartitions()) e = time.time() print(\u0026#34;Training time = {}\u0026#34;.format(e - se)) your_float_variable = (e - se) comment = \u0026#34;Training time for getnumpartition:\u0026#34; # Open the file in append mode and write the comment and variable with open(\u0026#39;output.txt\u0026#39;, \u0026#39;a\u0026#39;) as f: f.write(f\u0026#34;{comment} {your_float_variable}\\n\u0026#34;) Related Entries # Run plotly in JupyterLab "},{"id":6,"href":"/notebook/posts/vscode/","title":"VS Code Configuration \u0026 Set-up","section":"Knowledge Base","content":" Configuration # Remote SSH # Host machine Hostname machine.com User user_name IdentityFile path/to/ssh/key Remote SSH - SSH Tunnel # Host tunnel_machine Hostname machine.com User user_name IdentityFile path/to/ssh/key Host machine_after_tunnel Hostname machine_after_tunnel.com User user_name IdentityFile path/to/ssh/key ForwardAgent yes ProxyJump tunnel_machine PC Configuration # Authorize your windows local machine to connect to remote machine.\n$USER_AT_HOST=\u0026#34;your-user-name-on-host@hostname\u0026#34; $PUBKEYPATH=\u0026#34;$HOME\\.ssh\\id_ed25519.pub\u0026#34; $pubKey=(Get-Content \u0026#34;$PUBKEYPATH\u0026#34; | Out-String); ssh \u0026#34;$USER_AT_HOST\u0026#34; \u0026#34;mkdir -p ~/.ssh \u0026amp;\u0026amp; chmod 700 ~/.ssh \u0026amp;\u0026amp; echo \u0026#39;${pubKey}\u0026#39; \u0026gt;\u0026gt; ~/.ssh/authorized_keys \u0026amp;\u0026amp; chmod 600 ~/.ssh/authorized_keys\u0026#34; Verify that the authorized_keys file in the .ssh folder for your remote user on the SSH host is owned by you and no other user has permission to access it.\nUses # Extensions # Remote Explorer Docker "},{"id":7,"href":"/notebook/posts/hugo-website/","title":"Building a website using Hugo and Hosting it on GitHub Pages","section":"Knowledge Base","content":" Installations # Install Git - Link Install Hugo - Link Configuration # To create a new Hugo website, run: hugo new site mynewsite then cd to the directory cd mynewsite Initialize the site as a git repository git init Choose the hugo theme that suits you. Hugo offer a selection of themes developed by the community. This site for example was built using Hugo-Book. Add the theme as a submodule # For example: git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book Add the theme to your site configuration file # Could be config.toml OR config.yaml OR hugo.toml OR hugo.yaml echo \u0026#34;theme = \u0026#39;hugo-book\u0026#39;\u0026#34; \u0026gt;\u0026gt; config.toml You will be able to see a first version of your website locally by running: hugo server --minify Edit your configuration file baseURL = \u0026#39;http://example.org/\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My New Hugo Site\u0026#39; Theme ConfigurationGuidelines\nThemes\u0026rsquo; publishers offer guidelines to configure your webiste in accordance to the theme. Check your theme publisher page on hugo themes or their theme github repo for guidance and help. Hosting on Github Pages # On your project settings, go to Pages. You\u0026rsquo;ll be able to see your site\u0026rsquo;s link. Choose a Build and deployment source (Github actions OR deploy from branch). You can also choose to publish it on a custom domain. Edit your configuration file baseURL = \u0026#39;https://username.github.io/repository\u0026#39; languageCode = \u0026#39;en-us\u0026#39; title = \u0026#39;My New Hugo Site\u0026#39; theme = \u0026#39;hugo-book\u0026#39; Other Great Tools For Building Static Websites # Sphinx https://www.sphinx-doc.org/en/master/index.html VuePress https://vuepress.vuejs.org/ Read the docs https://about.readthedocs.com/features/ "},{"id":8,"href":"/notebook/posts/dash-jupyterlab/","title":"Run plotly in JupyterLab","section":"Knowledge Base","content":" 1 pip uninstall plotly 2 jupyter labextension uninstall @jupyterlab/plotly-extension 3 jupyter labextension uninstall jupyterlab-plotly 4 jupyter labextension uninstall plotlywidget 5 jupyter labextension update --all 6 pip install plotly==5.17.0 7 pip install \u0026#34;jupyterlab\u0026gt;=3\u0026#34; \u0026#34;ipywidgets\u0026gt;=7.6\u0026#34; 8 pip install jupyter-dash 9 jupyter labextension list Useful Links # What is Right extension for Plotly in JupyterLab? https://stackoverflow.com/questions/62604893/what-is-right-extension-for-plotly-in-jupyterlab https://jupyter-docker-stacks.readthedocs.io/en/latest/ https://github.com/jupyter/docker-stacks https://github.com/plotly/plotly.py "},{"id":9,"href":"/notebook/posts/prompt-engineering/","title":"Prompt Engineering","section":"Knowledge Base","content":" RAG Architecture # "},{"id":10,"href":"/notebook/posts/offline_pip/","title":"Install python packages offline","section":"Knowledge Base","content":"1- Download packages locally using a requirements file or download a single package\npip download -r requirements.txt ## Example - single package python -m pip download \\ --only-binary=:all: \\ --platform manylinux1_x86_64 --platform linux_x86_64 --platform any \\ --python-version 39 \\ --implementation cp \\ --abi cp39m --abi cp39 --abi abi3 --abi none \\ scipy 2- Copy them to the a temporary folder in your remote machine 3- On your machine, Activate conda and then install them using pip - specify installation options\nconda activate base ## Example of pip install pip install scipy-1.7.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl "},{"id":11,"href":"/notebook/posts/pyspark-jupyter-docker/","title":"Running PySpark \u0026 Jupyter With Docker","section":"Knowledge Base","content":"Thanks to the Jupyter community, it\u0026rsquo;s now much easier to run PySpark on Jupyter using Docker. There are two ways you can do this : 1. the \u0026ldquo;direct\u0026rdquo; way and 2. the customized way.\nThe \u0026ldquo;direct\u0026rdquo; way # verify your local settings are aligned with the pre-requisites to run this container, grosso modo make sure docker is installed, of course ! You have to have about 4 GB of free space pull image from docker hub https://hub.docker.com/r/jupyter/pyspark-notebook\nrun the downloaded image as a container - and do not forget to specify the volume to be used (to save and move the notebooks in and out the container) For more on the run command you can check the documentation\nyou can now access Jupyter notebook via the token provided by the image\nnow, your environment is ready for work !\nNote that if you install docker desktop, you can start/access/stop your container directly from the containers tab.\nThe customized way # The pyspark Jupyter image can also be used as a base image to build your own containers and run the environment that works best for your project.\nYou can check the Jupyter Docker stacks to select the image you might be interested in.\nwrite your Dockerfile build your container run your environement I\u0026rsquo;ve been working with a customized image for quite some time now. You can find it on my github.\n"},{"id":12,"href":"/notebook/posts/git-commands/","title":"Git commands I often use","section":"Knowledge Base","content":" Add # # only add files with .scala extension git ls-files [path] | grep \u0026#39;\\.scala$\u0026#39; | xargs git add git stash --keep-index "},{"id":13,"href":"/notebook/posts/code-documentation-scala/","title":"How to document your code?","section":"Knowledge Base","content":" Comment documenter ? # Les mêmes principes et critères d’un bon code devraient s’appliquer à la documentation:\nConventionnelle Simple Facile à comprendre En plus des critères d’un bon code, une bonne documentation devrait aussi être:\nExplicative (intention du code, règles métiers, clarification du code, mise en garde sur les conséquences d’une mauvaise utilisation, indications pour le testing) Non-redondante /** * Returns the temperature. */ int get_temperature(void) { return temperature; } Non-bruitée /** * Always returns true. */ public boolean isAvailable() { return false;} Bonnes pratiques # Introduire son code. # Décrire le contexte ou le background du code est une bonne pratique qui permettra aux lecteurs de se positionner par rapport aux conditions dans lesquelles le code a été généré et à ses objectifs.\nConnaître son public! # Avant de documenter, le développeur doit se poser la question: “qui sont/seront mes lecteurs?”. Une documentation visant un nettoyeur de données (focus ici sur les règles métiers par exemple) est différente d’une documentation visant un analyste de donnée (focus sur comment utiliser le code par exemple) Alerter sur les dépendances. Lister et décrire les perspectives du code.\n“No comment out”, i.e. Ne jamais retirer des lignes de documentation! # Si la documentation est bien faite et le code ne suit pas (à cause d’un refactor ou d’une modification), cela veut dire que le code ne fonctionne plus et qu’il devrait être supprimé ou lourdement remanié.\nÊtre gentil # Être gentil en laissant des exemples! Nice developers leave nice examples :)\nKID - keep it documented! # Un refactor ou modification sans revue de documentation veut juste dire que le développeur n’a rien compris à ce document !\nQuelques bons exemples de documentation # Instruction sur l’utilisation et renseignement sur le format de l’output # /** * Returns the temperature in tenth degrees Celsius * in range [0..1000], or -1 in case of an error. * * The temperature itself is set in the periodically * executed read_temperature() function. * * Make sure to call init_adc() before calling this * function here, or you will get undefined data. */ int get_temperature(void) { return temperature; } Contextualisation du code et restriction sur son utilisation # /** * * This is for testing purposes only and should * never be called from a real program. */ int get_temperature(void) { return temperature;} "},{"id":14,"href":"/notebook/posts/hive/","title":"Hive","section":"Knowledge Base","content":" Snippets # -- set identifiers to none for the query below to work and -- set it back to column once it\u0026#39;s done set hive.support.quoted.identifiers = none; HIVE 3 # BI Code typically use db.table - needs to change to db.table Default path : /warehouse/tablespace/external/hive/default.db/test_table Resources \u0026amp; Useful Links # ACID + HIVE\n"},{"id":15,"href":"/notebook/docs/data-science/churn/churn-management/","title":"Churn Management","section":"Data Science","content":"version du 25-12-2018\nGestion de l\u0026rsquo;attrition ou Churn management # Le churn est un anglicisme qui désigne la perte des clients. La gestion du churn figure parmi les piliers les plus importants à tenir en compte lors de la gestion de fidélisation, car l’acquisition à elle seule ne peut pallier aux pertes de la clientèle. (voir illustration)\nÉtudier le churn pousse à réfléchir parallèlement à plusieurs actions pouvant contribuer à sa réduction et à la maximisation de la valeur de la relation client:\nDéfinir des stratégies anti-attrition Mettre en place des actions de prévention de churn Affiner les ciblages Optimiser les efforts de rétention Le churn est un risque normal et fréquent qui interrompt la relation client/entreprise. Ce risque pourrait apparaître pendant différents stades du parcours client et requiert, par conséquent, des actions de retention adaptées à sa nature, ajustées au stade de la relation et personnalisés selon le segment du client.\nPre-Churn \u0026amp; Prediction du Churn # Une étude de churn s’effectue sur deux axes: l’analyse du churn et la prédiction du risque de churn.\nAnalyse des anciens churners : Analyser les différents comportements des clients avant l’événement de churn, Repérer des points d’inflexion partagés pouvant déclencher le churn, Détecter des segments postentiels de churners. Prédiction du risque de churn : Anticiper régulièrement la perte de clients, Définir un intervalle de temps sur lequel l\u0026rsquo;identification de churners potentiels se fera, Arrêter un seuil de risque nous permettant d’alerter contre le churn. "},{"id":16,"href":"/notebook/docs/data-science/churn/churn-problem-framing/","title":"Churn Problem Framing","section":"Data Science","content":" Churn - Problem Framing # Churn reporting # What happened? (Inactive point of view)\nIn general cohorts work good here: start with a small population of churners and follow them over time Clustering churners based on project and modeling bottom line Example of small population of churners =\u0026gt; “Churners who generate more profits, churns who explicitly expressed their insatisfaction”: or choose another prioritization criterion Monetary retention rates (monetary churn) Reversed cohorts: start with the churn date (Month / Year) and follow the churners retrospectively Product / offers / segments cohorts Maybe change of nomenclature/perspective to make problem framing more accessible to other shareholders (Product Churn, Account Churn, Usage Churn\u0026hellip;etc) Why did it happen?\nEvents that lead to customer churn (What are their existing pain points? Start with the possible obvious ones: bad customer service, non stop service, bad onboarding, no ongoing customer success plans, customers not using their accounts) What factors are driving churn? What are customer attrition rates? Does customer tenure differ for group A vs. B? Survival modeling (Cox, Nelson Aalen) -\u0026gt; Interpretable models, cohort comparison, time-specific predictions Are there any returning customers?\nWhy are they returning? What are the recapture triggers? What did they need to overcome their pain points? What are the return dates? Is there a trend? A seasonality? CLT Events? Do they have multiple churn dates? (are there any customers who churned after their return) Churn inference # What kinds of models to test? classification methods survival regression methods latent probability models graph neural networks How test these models for accuracy? How often should the models be refreshed? What could be automated? How long before new user behavior is reflected in predictions? Is there a distinction that should be taken into account from business stakeholders? Can some model or label drifts be anticipated ? What is the process to incorporate new streams of data? Are predictions actionable at the individual user level or only as a coarse segmentation? How will predictions learn from implemented users retention actions? What is the lift seen when using churn predictions? How is that lift augmented after implemented users retention actions? What is the Probability Threshold? How was is set? Churn monitoring # What is happening right now? (define indicators, metrics to monitor churn status) What is the monitoring frequency that we should be employing? Use descriptive models (not good for trend analysis, but good for summarizing what happened) Compare/monitor different time periods (Beta distributions models are good enough) Adaptive churn analysis and prediction # Reactive churn management Rigorous A/B testing Rentention analysis (What really generates retention, Retention based on Customer value - define customer value in context and problem scope) Proactive churn management "},{"id":17,"href":"/notebook/docs/data-science/feature-engineering/feature-store/","title":"Feature Store","section":"Data Science","content":"version 19-08-2024\nFeature Store as a System # follows a logical flow seperates concerns functionally builds modular-ily Pipeline and code break-down # Data Selection and UC-specific processing # We start by filtering a reference table based on specific conditions aligned with the requirements of the use case. This creates a use case table on which we select and process pivot columns.\nNote: This should happen in the processing and modeling modules. Pivot transformations and Data federation # We select specific columns in order to create transitional tables with the desired pivot columns.\nWe combine all transitional tables using union operations to create a single base table on which the features calculations will occur.\nNote: This base table must have the structure: clientId, pivotCol, valueCol, and eventDateCol. Features generation # The code defines functions for generating features with different parameters. The developer chooses which generator or generators to use and applies the generator on the previously created base table (the one created in phase 2.3)\nMemory management # Usage of memory management operations will be necessary. The developer must keep in mind the necessity of optimizing memory usage, especially for large datasets.\nPipeline and code best practices # Naming conventions and Nomenclatures # Variable names are descriptive Adhere to Scala conventions as much as possible Keep the code modular # Externalize configurations to make the code more flexible and easier to maintain Refactor duplicated codes Create specific objects, when needed, to federate segments of operations and functions Keep the code readable # Add inline comments, if necessary, explaining the purpose of each major operation to improve readability Add scala documentation to explain the purposes of functions, their usage and requirements Adhere to naming conventions Error handling and Unit testing # Consider adding try-catch blocks or other error handling mechanisms, especially for operations that might fail (like reading from or writing to external sources) Unit test operations and transformations that are key to the success of the pipeline "},{"id":18,"href":"/notebook/docs/data-science/outlier-detection/","title":"Outlier Detection","section":"Data Science","content":" Types of outliers # With regards to the distribution # Univariate: can be found when looking at a distribution of values in a single feature space. Multivariate: can be found in a n-dimensional space (of n-features). With regards to the environment # Point outliers: single data points that lay far from the rest of the distribution. Contextual outliers: can be noise in data, such as punctuation symbols when realizing text analysis or background noise signal when doing speech recognition. Collective outliers: Collective outliers can be subsets of novelties in data such as a signal that may indicate the discovery of new phenomena. Most common causes of outliers on a data set # Data entry errors (human errors) Measurement errors (instrument errors) Experimental errors (data extraction or experiment planning/executing errors) Intentional (dummy outliers made to test detection methods) Data processing errors (data manipulation or data set unintended mutations) Sampling errors (extracting or mixing data from wrong or various sources) Natural (not an error, novelties in data) Hint\nOutliers that are not a product of an error are called novelties.\nBefore starting the detection of outliers # Which and how many features am I taking into account to detect outliers ? (univariate / multivariate) Can I assume a distribution(s) of values for my selected features? (parametric / non-parametric) Most popular methods for outlier detection # Z-Score or Extreme Value Analysis (parametric): a metric that indicates how many standard deviations a data point is from the sample’s mean, assuming a gaussian distribution. Dbscan (Density Based Spatial Clustering of Applications with Noise): applied to detect outliers in nonparametric distributions in many dimensions. it is focused on finding neighbors by density (MinPts) on an ‘n-dimensional sphere’ with radius ɛ. A cluster can be defined as the maximal set of ‘density connected points’ in the feature space. Isolation forests: isolation forests are an effective method for detecting outliers or novelties in data. Isolation forest’s basic principle is that outliers are few and far from the rest of the observations. Probabilistic and Statistical Modeling (parametric) Linear Regression Models (PCA, LMS) Proximity Based Models (non-parametric) Information Theory Models High Dimensional Outlier Detection Methods (high dimensional sparse data) Anomaly detection Literature # Statistical AD techniques: fit a statistical model for normal behavior Density-based - ex: Local Outlier Factor (LOF) and variantes (COF ODINLOCI) Support estimation - OneClassSVM High-dimensional techniques: - Spectral Techniques - Random Forest - Isolation Forest "},{"id":19,"href":"/notebook/docs/data-science/sna/sna/","title":"SNA","section":"Data Science","content":" "},{"id":20,"href":"/notebook/docs/deep-learning/logregnn/logregnn/","title":"Log Reg Nn","section":"Deep Learning","content":" Logistic Regression as a Neural Network # Architecture # 𝐺𝑖𝑣𝑒𝑛 𝑥 , 𝑦̂ = 𝑃(𝑦 = 1|𝑥), where 0 ≤ 𝑦̂ ≤ 1\nParameters of logistic regression\nInput observation,features matrix X Target vector Y Weights w Threshold or bias b Output: 𝑦̂, sigmoid(z) where z = 𝑤 𝑇 *𝑥 + 𝑏 To get the parameters w and b (i.e. learning), we optimize on:\n𝐽(𝑤, 𝑏) = 1/m (∑ 𝐿(𝑦̂ (𝑖) , 𝑦 (𝑖) ))\ni.e.\n𝐽(𝑤, 𝑏) = 1/m(∑ ylog((𝑦̂) + (1-y)(1-log(1-𝑦̂))))\nImplementation # github/Non-NeutralZero - CatVsNonCat.ipynb\n"},{"id":21,"href":"/notebook/docs/deep-learning/noprop/noprop/","title":"No Prop","section":"Deep Learning","content":" NOPROP # NOPROP: TRAINING NEURAL NETWORKS WITHOUT BACK-PROPAGATION OR FORWARD-PROPAGATION\nQinyu Li, Yee Whye Teh, Razvan Pascanu (arxiv) arXiv:2503.24322\nThe authors propose NoProp, a completely gradient-free method for training neural networks. No prop does not require back-prop or forwrad prop. Instead of learning via layer gradients, each layer is independently trained to denoise a noisy version of the label. And during inference, the noise is removed layer by layer.\nProblematic # Biological implausibility of back-propagation. Memory costs due to storing activations during forward pass to facilitate backward passes. Sequential dependencies of propagation hinders parallel computing. Catastrophic forgetting in continual learning. Ideation # The authors were inspired by recent advances in generative modeling, specifically diffusion models and flow matching methods. The key insight is to reconceptualize neural network training - which we can be broken down to:\nReframing the problem : denoising at each layer vs sequentially propagating information across layers. Fixing the representation at each layer beforehand to a noised version of the target. Questioning the assumption that hierarchical representations are necessary for effective learning. Contribution # Introducing NoProp and its variants.\nVariants\nDiscrete-Time NoProp (NoProp-DT): has a fixed number of denoising steps. Continuous-Time NoProp (NoProp-CT): learns a dynamic denoising process. Flow Matching (NoProp-FM): learns a vector field to carry noise to the label embedding. Modeling Considerations\nNoProp requires pre-fixing representations at each layer -\u0026gt; careful modeling and design. The authors experimented with different initialization strategies for the class embedding matrix, including one-hot vectors, orthogonal matrices, and prototype-based approaches. For the continuous-time variants, there was added complexity in conditioning on time. Validation # NoProp-DT performs on par or better than backprop on MNIST and CIFAR-10. NoProp variants outperform prior backprop-free methods like Forward-Forward (Hinton, 2022), Difference Target Propagation, Local Greedy Forward Gradient. NoProp uses less memory during training. "},{"id":22,"href":"/notebook/docs/machine-learning/decision-trees/","title":"Decision Trees","section":"Machine Learning","content":" Decision Trees # On each node, a decision tree asks a question and answers it with yes/no to classify observations under that node. The question could be based on a specific True/False (is obs.feature = something) question or could be based on numeric data (is obs.feature \u0026gt; 10) The classification can be categorical or numerical The first node is called the Root node, the nodes in the middle are called internal nodes, and the results are called leaf nodes "},{"id":23,"href":"/notebook/docs/machine-learning/explainability-snippets/","title":"Explainability Snippets","section":"Machine Learning","content":"version 2018\nWhy Model interpretation? # Understanding how a model makes decisions — model interpretation — has been on the front burner since the end of 2017. Decision support systems and models they are based on don’t explain which features influenced their decisions were known as black boxes. Model interpretability is not only important for companies that need to fulfill legal obligations to customers. It serves a technical purpose as well. Every ML model considers input features (problem properties) to predict results (outputs). The more relevant features we create and use to train an ML model during feature engineering, the more accurate results we can get and the simpler our model is. That’s why the ability to understand how the model makes predictions is crucial for its debugging. source\nWhat is model interpretability? # Machine learning models are very complex tools, so-called black-box classifiers, that don’t offer straightforward and human-interpretable decision rules.\nAs data scientists, we should be able to provide an explanation to end users about how a model works. However, this not necessarily means understanding every piece of the model or generating a set of decision rules. There could also be a case where this is not required:\nproblem is well studied, model results has no consequences, understanding the model by the end-user could pose a risk of gaming the system. source\nHow to do it? # Feature importance analysis Feature correlations Existing packages and tools (LIME, SHAP, Manifold\u0026hellip;) Model Interpretation Guidelines # Global Interpretability: How well can we understand the relationship between each feature and the predicted value at a global level — for our entire observation set. Can we understand both the magnitude and direction of the impact of each feature on the predicted value? Local Interpretability: How well can we understand the relationship between each feature and the predicted value at a local level — for a specific observation. Feature Selection: Does the model help us focus on only the features that matter? Can it zero out the features that are just “noise”? We can quickly identify that Feature X may be the most important, but does it make Outcome Y more or less likely? There may not be a yes-or-no answer.\nResources # Blogs and Forums # https://www.kdnuggets.com/2019/02/ai-data-science-advances-trends.html https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f http://www.cse.chalmers.se/~richajo/dit866/lectures/l4/Feature%20ranking%20examples.html https://blog.dominodatalab.com/shap-lime-python-libraries-part-1-great-explainers-pros-cons/ https://slundberg.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html https://forums.fast.ai/t/feature-importance-of-random-forest-vs-xgboost/17561/2 https://github.com/parrt/random-forest-importances https://github.com/scikit-learn/scikit-learn/pull/13146 https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7 https://www.datascience.co.il/blog https://christophm.github.io/interpretable-ml-book/ Papers # “Why Should I Trust You?” Explaining the Predictions of Any Classifier https://arxiv.org/pdf/1602.04938.pdf LIME - an Introduction https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Interpretable machine learning: definitions, methods, and applications https://arxiv.org/pdf/1901.04592.pdf https://web.stanford.edu/~hastie/Papers/ESLII.pdf "},{"id":24,"href":"/notebook/docs/machine-learning/random-forests/","title":"Random Forests","section":"Machine Learning","content":" Random Forest # Random forests are built from decision trees\nInitially, the original data is bootstrapped by randomly sampling the data and creating a new dataset with the same size as the original one (to be able to do that, duplicated obs are allowed - aka random sampling with replacement) Build a decision tree based on the bootsrapped data Randomly select features (typically sqrt(n_features)) from the bootsrapped data when splitting nodes (this is called random subspace method) Go back to step 1 and repeat does all the original data end up in the sampled subsets? For each created Decision Tree, the non-bootsrapped data is called Out-of-Bag data. once we get the forest, how do we use it? if we want to get a prediction, we run an obs through all the trees of the forest and pick the prediction with the most votes. (this process is called Bagging, i.e. Bootsrapping + aggregating single predictions) how do we evaluate the random forest? we can evaluate it using the out-of-bag error, i.e. measure how accurate the forest predicts out-of-bag data. is there an optimal number of features for each bootsrapped sample? Yes. Given that we can measure the out-of-bag error, we can use it to compare forests built on different samples of features and select the one with the smallest error. how many times should we repeat this processes? plot OOB error rate vs. number of trees Why are they called random forests? Because of the random sampling concept at step 1 and at step 3 how is a forest better than one decision tree? By getting a large number of different (high variance ) trees For more, see Chapter 15 in https://web.stanford.edu/~hastie/Papers/ESLII.pdf\n"},{"id":25,"href":"/notebook/docs/machine-learning/svm/","title":"Svm","section":"Machine Learning","content":" SVM # P: We want to figure out a way to separate data into classes S: A linear classifier can help. Its objective would be to divide data using a hyperplane, and since the data points, from each class, that are closer to the classifier will be helping us decide on the orientation and position of the classifier, we can give them a fancy name (Support vectors) and call our linear classifier, the Support Vector Classifier. P: But, If the data is \u0026ldquo;clearly\u0026rdquo; separable, then we won\u0026rsquo;t have one classifier - we would actually end up with so many possibilities to choosing one hyperplane AND we have to make sure future predictions are more likely to be correctly classified S: We could choose the one that insures the maximum distance between the Support vectors of the two classes (we\u0026rsquo;ll call it a maximum margin classifier.) That way, we\u0026rsquo;ll seperate the existing data, and have more confidence in classifying future data points. P: We\u0026rsquo;ll be looking to maximize the margin between the data points and the hyperplane, how do we do that? S: Hinge Loss! P: Overfitting, this type of classifier would be very sensitive to outliers for example S: Explorative data analysis, outliers analysis or allow misclassification (soft margin). o "}]